{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f64c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import src.config as config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c96952e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_date=Timestamp('2025-12-31 18:00:00+0000', tz='UTC')\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "current_date = pd.to_datetime(datetime.now(timezone.utc)).floor('h')\n",
    "print(f\"{current_date=}\")\n",
    "\n",
    "# we fetch raw data for the last 28 days, to add redundancy to our data pipeline\n",
    "fetch_data_to = current_date\n",
    "fetch_data_from = current_date - timedelta(days=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ac135",
   "metadata": {},
   "source": [
    "We need to fetch the recent data. We don't have access to NYC Association Data Warehouse. So we are going to simulate a call to a data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c14b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import load_raw_data\n",
    "\n",
    "def fetch_batch_raw_data(from_date: datetime, to_date: datetime) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Simulate production data by sampling historical data from 52 weeks ago (ie 1 year)\n",
    "    \"\"\"\n",
    "    from_date_ = from_date - timedelta(days=7*52)\n",
    "    to_date_ = to_date - timedelta(days=7*52)\n",
    "\n",
    "    # download 2 files from website\n",
    "    rides = load_raw_data(year=from_date_.year, months=from_date_.month)\n",
    "    rides['pickup_datetime'] = pd.to_datetime(rides['pickup_datetime']).dt.tz_localize(None)\n",
    "    rides = rides[rides.pickup_datetime >= from_date_.replace(tzinfo=None)]    \n",
    "    rides_2 = load_raw_data(year=to_date_.year, months=to_date_.month)\n",
    "    rides_2['pickup_datetime'] = pd.to_datetime(rides_2['pickup_datetime']).dt.tz_localize(None)\n",
    "    rides_2 = rides_2[rides_2.pickup_datetime < to_date_.replace(tzinfo=None)]\n",
    "\n",
    "    rides = pd.concat([rides, rides_2])\n",
    "\n",
    "    # Shift the data to pretend this is recent data\n",
    "    rides['pickup_datetime'] += timedelta(days=7*52)\n",
    "\n",
    "    rides.sort_values(by=['pickup_location_id', 'pickup_datetime'], inplace=True)\n",
    "\n",
    "    return rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329f2bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 2024-12 was already in local storage\n",
      "File 2025-01 was already in local storage\n"
     ]
    }
   ],
   "source": [
    "rides = fetch_batch_raw_data(from_date=fetch_data_from, to_date=fetch_data_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d25eb9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 259/259 [00:01<00:00, 170.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data import transform_raw_data_into_ts_data\n",
    "ts_data = transform_raw_data_into_ts_data(rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "555db04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOME\\anaconda3\\envs\\pyimg\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 12:50:10,065 INFO: Initializing external client\n",
      "2025-12-31 12:50:10,066 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.6.0 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 12:50:11,040 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1329302\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "# Connect to the project \n",
    "project = hopsworks.login(\n",
    "    project=config.HOPSWORKS_PROJECT_NAME,\n",
    "    api_key_value=config.HOPSWORKS_API_KEY\n",
    ")\n",
    "\n",
    "# Connect to the feature store\n",
    "feature_store = project.get_feature_store()\n",
    "\n",
    "# Connect to the feature group\n",
    "feature_group = feature_store.get_or_create_feature_group(\n",
    "    name=config.FEATURE_GROUP_NAME,\n",
    "    version=config.FEATURE_GROUP_VERSION,\n",
    "    description = \"Time series data at hourly frequency\",\n",
    "    primary_key = ['pickup_location_id', 'pickup_hour'],\n",
    "    event_time='pickup_hour'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1ffc463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 678321/678321 | Elapsed Time: 00:43 | Remaining Time: 00:00\n",
      "UserWarning: Materialization job is already running, aborting new execution.Please wait for the current execution to finish before triggering a new one.You can check the status of the current execution using `fg.materialization_job.get_state()`.or `fg.materialization_job.get_final_state()` or check it out in the Hopsworks UI.at https://c.app.hopsworks.ai:443/p/1329302/jobs/named/time_series_hourly_feature_group_1_offline_fg_materialization.\n",
      "Use fg.materialization_job.run(args=-op offline_fg_materialization -path hdfs:///Projects/nyc_taxiride_demand/Resources/jobs/time_series_hourly_feature_group_1_offline_fg_materialization/config_1767206643318) to trigger the materialization job again.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('time_series_hourly_feature_group_1_offline_fg_materialization', 'SPARK'),\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_data['pickup_location_id'] = ts_data['pickup_location_id'].astype('int32')\n",
    "feature_group.insert(\n",
    "    ts_data,\n",
    "    write_options={\n",
    "        \"wait_for_job\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f105865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyimg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
